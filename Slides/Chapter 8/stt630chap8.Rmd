---
title: "Chapter 8: Regression"
author: "Brandon M. Greenwell"
date: "October 30, 2017"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Introduction

A very common problem in scientific studies is to determine how one variable depends on one or more other variables. For example, is a hurricane’s wind speed related to the ocean water temperature? If so, what is the relationship?


## Introduction

Think back to the pelican egg example where data was collected on eggshell thickness and the level of PCB’s in the birds. A question of interest is whether or not the eggshell thickness depends on the PCB level and if so, how does the PCB level affect eggshell thickness? Again, regression analysis can be used to answer this question.


## Introduction

Francis Galton is credited with originating the term regression when he studied how heights of children depended on the heights of their parents. Can we predict a child’s height based on the height of the child’s parents? Or, for parents of a particular height, what is the average height of their offspring? Linear regression is used to answer these types of questions.


## Simple linear regression

Below is a scatterplot of time (in seconds) to run 2 miles versus maximum volume of $O_2$ uptake for a sample of middle-aged men.

```{r}
vol <- c(
  42.33, 53.1, 42.08, 50.06, 42.45, 42.46, 47.82, 49.92, 36.23, 49.66, 41.49, 
  46.17, 48.18, 43.21, 51.81, 53.28, 53.29, 47.18, 56.91, 47.8, 48.65, 53.69, 
  60.62, 56.73
)
time <- c(
  918, 805, 892, 962, 968, 907, 770, 743, 1045, 810, 927, 813, 858, 860, 760, 
  747, 743, 803, 683, 844, 755, 700, 748, 775
)
plot(vol, time, pch = 19, xlab = "Time (seconds)", ylab = "Volume")
```


## Simple linear regression

- In regression, we have a response variable $y$ (also known as the dependent variable) and a predictor variable $x$ (also known as the independent or regressor variable).

- Data is collected on pairs $\left(x_1, y_1\right), \left(x_2, y_2\right), \dots, \left(x_n, y_n\right)$.

- In regression, the average response depends on the value of $x$ which defines a
*conditional expectation* of $y$ given $x$, denoted $E\left[y|x\right]$.

- The way to think of this conditional expectation is: what is the average value of $y$ for a given value of $x$?


## Simple linear regression

- In simple linear regression, we model $E\left[y|x\right]$ as a linear function: $$E\left[y|x\right] = \beta_0 + \beta_1 x.$$

- More generally, we assume that $$y = \beta_0 + \beta_1 x + \epsilon,$$ where $\epsilon$ is a random variable with mean $0$ and variance $\sigma^2$.


## Simple linear regression

In regression analysis, the common statistical problems of interest are:

- to test if whether or not the predictor $x$ is associated with the response $y$;
- predict a response for a given value of $x$;
- estimate an average response for a given value of $x$;
- model the relation between $x$ and $y$.


## Simple linear regression

```{r}
vol <- c(
  42.33, 53.1, 42.08, 50.06, 42.45, 42.46, 47.82, 49.92, 36.23, 49.66, 41.49, 
  46.17, 48.18, 43.21, 51.81, 53.28, 53.29, 47.18, 56.91, 47.8, 48.65, 53.69, 
  60.62, 56.73
)
time <- c(
  918, 805, 892, 962, 968, 907, 770, 743, 1045, 810, 927, 813, 858, 860, 760, 
  747, 743, 803, 683, 844, 755, 700, 748, 775
)
plot(vol, time, pch = 19, xlab = "Time (seconds)", ylab = "Volume")
abline(lm(time ~ vol), lwd = 2, col = "red")
```


## Fitting regression lines: least squares

**Question:** How should we estimate the regression line?

- While there are a number of ways to estimate $\beta_0$ and $\beta_1$, the most common approach is *least squares estimation*.

- In least squares, the objective is to minimize the sum of squared errors $$\left[y - \left(\beta_0 + \beta_1 x\right)\right] ^ 2$$


## Fitting regression lines: least squares

- Let $\widehat{\beta}_0$ and $\widehat{\beta}_1$ represent the least squares estimates of $\beta_0$ and $\beta_1$, respectively.

- Calculus can be used to show that $$\widehat{\beta}_1 = \frac{\sum_{i=1}^n\left(x_i - \bar{x}\right)\left(y_i - \bar{y}\right)}{\sum_{i=1}^n\left(x_i - \bar{x}\right) ^ 2}$$
and $$\widehat{\beta}_0 = \bar{y} - \widehat{\beta}_1 \bar{x}$$


## Simple linear regression

- For a given data point $\left(x_i, y_i\right)$, the predicted response, denoted $\widehat{y}_i$, is given by $$\widehat{y}_i = \widehat{\beta}_0 + \widehat{\beta}_1 x_i$$.


## Slide with Plot

```{r pressure}
plot(pressure)
```

